// The Big Picture
// A synthesis of surveillance trends and their combined implications

EXTERNAL navigateTo(storyId)
EXTERNAL exit()

-> Start

=== Start ===

# diagram: ../digital-id/surveillance_blueprint.jpg

Digital ID. Age verification. Facial recognition. On-device scanning. VPN bans.

These aren't separate policies. They're pieces of a puzzle.

Each one seems reasonable in isolation. Together, they create infrastructure for something else entirely.

+ [What's the pattern?]
    -> The_Pattern

+ [Who's pushing for this?]
    -> Who_Benefits

+ [I've got nothing to hide]
    -> Nothing_To_Hide

+ [Where does this lead?]
    -> Where_This_Leads

=== The_Pattern ===

# diagram: ../on-device-scanning/scanning_layers.jpg

Every policy follows the same template:

• Start with child safety or fraud prevention—goals everyone supports
• Require identity verification to access services
• Build databases linking real identities to online activity
• Expand scope once infrastructure exists

The Children's Wellbeing Bill alone contains provisions for unique child identifiers, social media bans, age verification, VPN restrictions, and on-device scanning.

That's not coincidence. That's architecture.

+ [Show me the infrastructure]
    -> The_Infrastructure

+ [But each policy has good intentions?]
    -> Good_Intentions

+ [I've got nothing to hide]
    -> Nothing_To_Hide

=== Who_Benefits ===

# diagram: ../digital-id/policy_framework.jpg

This section is speculative—but the financial interests behind digital ID are worth examining.

Digital identity systems are enormously lucrative. Governments pay billions to build and maintain them. The companies that win these contracts gain long-term revenue and access to sensitive infrastructure.

So who's advocating loudest?

+ [Tony Blair and the Blair Institute]
    -> Blair_Institute

+ [Oracle and Larry Ellison]
    -> Oracle_Connection

+ [The World Economic Forum]
    -> WEF_Initiative

+ [Back to the pattern]
    -> The_Pattern

=== Blair_Institute ===

# diagram: ../digital-id/companies_house.jpg

Tony Blair has been the UK's most prominent digital ID advocate.

His Tony Blair Institute for Global Change has published multiple reports arguing digital ID would save £1.25 billion annually and "modernise" public services. [22]

Blair admits the UK public will "need a little persuading"—perhaps remembering his own government's failed ID card scheme. [23]

The Institute claims 62% of Britons support digital identity—but critics note this doesn't mean support for a *mandatory* system. [24]

The current government's digital ID push wasn't in Labour's election manifesto. A petition against mandatory digital ID has gathered nearly 3 million signatures. [25]

+ [What about Oracle?]
    -> Oracle_Connection

+ [What about the WEF?]
    -> WEF_Initiative

+ [Back to the pattern]
    -> The_Pattern

=== Oracle_Connection ===

# diagram: ../digital-id/tech_stack.jpg

Oracle, the tech giant founded by Larry Ellison, holds over £1 billion in UK government contracts. [26]

Reports indicate Oracle is positioned as a frontrunner to provide infrastructure for the UK's "One Login" digital ID system.

Here's where it gets interesting: Larry Ellison's Ellison Institute has donated or pledged approximately £257 million to the Tony Blair Institute. [27]

There have been numerous meetings between Oracle, the Ellison Institute, and senior government officials. Critics describe this as a "revolving door" between political influence and corporate profit. [28]

This doesn't prove wrongdoing. But it raises questions about who benefits when digital ID becomes policy.

+ [What about the WEF?]
    -> WEF_Initiative

+ [What about Blair?]
    -> Blair_Institute

+ [Back to the pattern]
    -> The_Pattern

=== WEF_Initiative ===

# diagram: ../digital-id/regulatory_framework.jpg

The World Economic Forum has been promoting digital identity globally since at least 2018.

Key initiatives include:

• Known Traveller Digital Identity (KTDI) — A system linking biometrics, digital identity, and consent-based data sharing for international travel [29]

• ID2020 Alliance — A partnership promoting "ethical" digital ID, merged with Digital Impact Alliance in 2023 [30]

• "Reimagining Digital ID" report (2023) — Describes digital identity as essential infrastructure for the future [31]

These initiatives align with UN Sustainable Development Goals targeting universal digital ID by 2030.

The WEF frames this as empowering individuals. Critics see it as building global surveillance infrastructure with a friendly face.

+ [What about digital currency?]
    -> Digital_Currency

+ [What about Blair?]
    -> Blair_Institute

+ [What about Oracle?]
    -> Oracle_Connection

+ [Back to the pattern]
    -> The_Pattern

=== Digital_Currency ===

# diagram: ../digital-id/digital_wallet.jpg

Digital identity becomes even more powerful when linked to digital currency.

The Bank of England is developing a "digital pound"—sometimes called Britcoin. A decision on whether to proceed is expected in late 2025 or 2026. [32]

The technology allows for "programmable money"—currency with built-in rules. Money that expires if not spent. Spending limits. Automatic restrictions. [33]

The Bank says this won't be used for surveillance or control. But the capability would exist.

+ [How would this work in practice?]
    -> CBDC_Concerns

+ [Has this been done elsewhere?]
    -> China_CBDC

+ [Back to who benefits]
    -> Who_Benefits

=== CBDC_Concerns ===

# diagram: ../on-device-scanning/government_control.jpg

Here's what programmable digital currency could enable:

• Spending restrictions — Currency that can only be used for approved purposes

• Expiration dates — Stimulus payments that vanish if not spent quickly

• Holding limits — The Bank proposes £10,000-£20,000 caps per person [34]

• Real-time tracking — Every transaction visible to intermediaries

The Bank of England insists it won't monitor spending. But public consultation responses showed "strong mistrust of the authorities' motives." [33]

Once the infrastructure exists, future governments could change the rules.

+ [What about China?]
    -> China_CBDC

+ [Back to who benefits]
    -> Who_Benefits

+ [Back to the pattern]
    -> The_Pattern

=== China_CBDC ===

# diagram: ../age-verification/great_firewall.jpg

China's digital yuan shows what's possible.

Over 260 million wallets have been opened. The currency is used for salaries, transport, and retail. [35]

It's fully programmable. Stimulus payments have been issued that expire if not spent within weeks.

Most concerning: it integrates with China's social credit system. Citizens on blacklists—for unpaid fines, "spreading misinformation," or other infractions—can have their digital wallets frozen or suspended. [36]

Every transaction is tracked in real time by central authorities.

The UK isn't China. But we're considering the same technology.

+ [Back to who benefits]
    -> Who_Benefits

+ [Back to the pattern]
    -> The_Pattern

+ [Where does this lead?]
    -> Where_This_Leads

=== The_Infrastructure ===

# diagram: ../digital-id/surveillance_layers.jpg

Here's what's being built:

• Digital ID creates a verified identity linked to biometrics

• Age verification requires you to prove that identity to access websites

• Facial recognition tracks you in physical spaces

• On-device scanning monitors your private messages before encryption

• VPN bans prevent you from hiding your location or browsing

Combined, these systems can track what you read, who you talk to, where you go, and what you say—all tied to your real identity.

+ [Has this been done before?]
    -> China_Example

+ [But we have safeguards?]
    -> Safeguards

+ [I've got nothing to hide]
    -> Nothing_To_Hide

=== China_Example ===

# diagram: ../age-verification/great_firewall.jpg

China's social credit system didn't arrive overnight.

It was built piece by piece: identity verification, facial recognition in public spaces, monitoring of online activity, restrictions on VPNs.

Each piece had a stated purpose. Fraud prevention. Public safety. Social harmony.

The result? Citizens can be denied train tickets, loans, or jobs based on their "score." Attending protests, posting criticism, or associating with the wrong people affects that score. [1]

The UK isn't China. But we're building the same toolkit.

+ [Could that happen here?]
    -> Could_Happen_Here

+ [Our government wouldn't do that]
    -> Government_Trust

+ [Back to the pattern]
    -> The_Pattern

=== Could_Happen_Here ===

# diagram: ../on-device-scanning/mission_creep_examples.jpg

Consider what's already legal in the UK:

The Investigatory Powers Act allows access to your "internet connection records"—a log of every website you visit—without a warrant. [2]

Police facial recognition databases include 19 million custody images, including "vast numbers of photos of innocent people." [3]

The Online Safety Act allows the government to require platforms to scan private messages. [4]

These aren't hypotheticals. They're current law.

+ [But laws can be challenged?]
    -> Laws_Change

+ [I still have nothing to hide]
    -> Nothing_To_Hide

+ [What can I do?]
    -> Take_Action

=== Good_Intentions ===

# diagram: ../on-device-scanning/false_choice.jpg

Of course each policy has good intentions.

Protecting children is vital. Reducing fraud matters. Catching criminals is important.

The question isn't whether these goals are worthy. It's whether these methods achieve them—and at what cost.

Age verification pushed users to unregulated sites where protections don't exist. [5]

Facial recognition wrongly flagged an elderly woman as a shoplifter, leaving her "scared to go shopping alone." [6]

On-device scanning would create false positives affecting thousands of innocent people. [7]

Good intentions don't guarantee good outcomes.

+ [Tell me about false positives]
    -> False_Positives

+ [What about scope creep?]
    -> Scope_Creep

+ [I've got nothing to hide]
    -> Nothing_To_Hide

=== Nothing_To_Hide ===

# diagram: ../on-device-scanning/chilling_effect.jpg

"I've got nothing to hide" assumes three things:

1. Laws won't change
2. Systems won't make mistakes
3. You'll never be targeted

All three assumptions are wrong.

+ [Laws change]
    -> Laws_Change

+ [Systems make mistakes]
    -> False_Positives

+ [Anyone can be targeted]
    -> Targeting

=== Laws_Change ===

# diagram: ../on-device-scanning/expanding_definitions.jpg

What's legal today may not be tomorrow.

Homosexuality was illegal in the UK until 1967. Attending certain protests could become illegal under expanded public order laws.

Surveillance infrastructure built today will be used by governments you haven't elected yet, enforcing laws that don't exist yet.

In 2020, Hong Kong's national security law criminalised speech that had been legal the day before. Protesters who had been photographed at legal demonstrations were suddenly criminals. [8]

Data collected for one purpose gets repurposed. That's not paranoia—it's documented history.

+ [Tell me about false positives]
    -> False_Positives

+ [What about targeting?]
    -> Targeting

+ [What can I do?]
    -> Take_Action

=== False_Positives ===

# diagram: ../facial-recognition/misidentification.jpg

These systems make mistakes. At scale, mistakes affect thousands.

Facial recognition — 80% of people misidentified by police in London in 2025 were Black. A community volunteer was wrongly stopped at London Bridge. [3]

Retail bans — A 64-year-old woman was banned from shops after being wrongly flagged for stealing paracetamol worth less than £1. [6]

Age verification — Roblox's AI system classified 10-year-olds as adults and adults as teenagers. [9]

Photo scanning — A father was investigated by police and had his Google account deleted after sending medical photos of his son to a doctor. The AI flagged them as abuse material. [7]

You don't have to do anything wrong to be wrongly accused.

+ [What about deliberate targeting?]
    -> Targeting

+ [What about racial bias?]
    -> Racial_Bias

+ [What can I do?]
    -> Take_Action

=== Targeting ===

# diagram: ../on-device-scanning/government_control.jpg

Surveillance infrastructure can be weaponised.

Hash collisions — The same scanning technology proposed for detecting abuse material could flag innocent images that happen to match. Researchers have demonstrated this is possible. Someone could send you an image designed to trigger a match. [10]

Political targeting — Browsing history tied to real identity means knowing who reads what. Protest organisers. Journalists' sources. Political opponents.

Blackmail potential — Any database of sensitive information—age-verified adult site visits, medical records, location history—is a blackmail database if breached. [11]

You might trust the current government. But this infrastructure will exist for decades.

+ [Tell me about the chilling effect]
    -> Chilling_Effect

+ [Has this happened in democracies?]
    -> Democracy_Examples

+ [What can I do?]
    -> Take_Action

=== Racial_Bias ===

# diagram: ../facial-recognition/racial_bias.jpg

These systems don't fail equally.

Facial recognition false positive rates for Black women are up to 247 times higher than for white subjects. [12]

Age estimation is less accurate for people with darker skin tones. [13]

When flawed systems are deployed at scale, marginalised communities bear the brunt of errors.

This isn't theoretical. It's measured, documented, and ongoing.

+ [Back to targeting]
    -> Targeting

+ [What can I do?]
    -> Take_Action

=== Scope_Creep ===

# diagram: ../on-device-scanning/scope_creep_stages.jpg

Every surveillance power expands beyond its original purpose.

RIPA 2000 was passed for serious crime and terrorism. Councils used it to investigate dog fouling and school catchment fraud. [14]

Facial recognition was trialled for serious crime. Now it's in corner shops flagging suspected shoplifters.

Communications data access was meant for national security. Over 500,000 requests are made annually, mostly for minor matters. [2]

On-device scanning starts with child abuse material. The EU proposal already includes "grooming" detection—analysing the content of your conversations. [15]

Once infrastructure exists, scope expands. Always.

+ [What's the end state?]
    -> Where_This_Leads

+ [I've got nothing to hide]
    -> Nothing_To_Hide

+ [What can I do?]
    -> Take_Action

=== Chilling_Effect ===

# diagram: ../on-device-scanning/chilling_effect.jpg

Surveillance changes behaviour even when you've done nothing wrong.

Knowing you're watched makes you cautious. You self-censor. You don't search for certain topics. You don't attend certain events. You don't contact certain people.

Studies show people change their online behaviour when they believe they're being monitored—even if they have "nothing to hide." [16]

A society where everyone watches what they say isn't a free society. It's a society that has internalised control.

+ [What's the end state?]
    -> Where_This_Leads

+ [What can I do?]
    -> Take_Action

=== Democracy_Examples ===

# diagram: ../digital-id/echr_article_8.jpg

Surveillance overreach isn't limited to authoritarian regimes.

USA — The NSA's mass surveillance was only revealed by a whistleblower. The infrastructure existed in a democracy. [17]

UK — Undercover police infiltrated environmental groups and anti-racism campaigns for decades, forming relationships and fathering children under false identities. [18]

Australia — Police used COVID contact tracing data for criminal investigations—despite promises it would only be used for public health. [19]

Democratic governments overreach too. The difference is whether infrastructure makes overreach easy or hard.

+ [What's the end state?]
    -> Where_This_Leads

+ [What can I do?]
    -> Take_Action

=== Where_This_Leads ===

# diagram: ../digital-id/surveillance_blueprint.jpg

If current trends continue:

• Accessing any online service requires verified identity
• Facial recognition tracks movement in public spaces
• On-device scanning monitors private communications
• VPNs are blocked or criminalised
• All data links to a single identifier

This creates a society where everything you do—online and offline—is recorded, linked to your real identity, and available to government and corporations.

Not because anyone planned dystopia. Because each small step seemed reasonable, and no one saw the whole picture.

+ [Is this inevitable?]
    -> Not_Inevitable

+ [What can I do?]
    -> Take_Action

=== Not_Inevitable ===

# diagram: ../digital-id/fork_in_road.jpg

It's not inevitable.

The EU has banned real-time facial recognition in public spaces (with exceptions). [20]

Courts have ruled against disproportionate surveillance.

Public pressure has delayed and modified proposals.

Technology can protect privacy—end-to-end encryption exists because people demanded it.

But these protections require active defence. Surveillance infrastructure, once built, is rarely dismantled.

+ [What can I do?]
    -> Take_Action

+ [Back to start]
    -> Start

=== Safeguards ===

# diagram: ../facial-recognition/ico.jpg

Safeguards exist on paper. Enforcement is another matter.

The ICO learned about racial bias in police facial recognition from news reports—not from the government. [12]

GOV.UK One Login was breached by a security test without being detected. [21]

The government proposes surveillance powers while simultaneously opposing encryption that would protect citizens from criminals.

Safeguards only work if they're enforced. And enforcement requires knowing what's happening.

+ [What about scope creep?]
    -> Scope_Creep

+ [What can I do?]
    -> Take_Action

=== Government_Trust ===

# diagram: ../on-device-scanning/ratchet_effect.jpg

You might trust the current government.

But infrastructure outlasts governments.

Powers granted for one purpose get used for others. Data collected today will exist for decades. Systems built for safety get repurposed for control.

The question isn't whether you trust this government. It's whether you trust every future government with a complete record of your life.

+ [Show me historical examples]
    -> Democracy_Examples

+ [What's the end state?]
    -> Where_This_Leads

+ [What can I do?]
    -> Take_Action

=== Take_Action ===

# diagram: ../digital-id/take_action.jpg

Awareness is the first step. Action is the next.

• Respond to consultations — The Home Office facial recognition consultation closes February 2026. Your voice matters.

• Contact your MP — Ask specific questions about digital ID, scanning powers, and surveillance safeguards.

• Support organisations — Big Brother Watch, Open Rights Group, and Liberty challenge these policies legally and politically.

• Talk to others — Most people don't know what's being built. Share what you've learned.

• Use privacy tools — End-to-end encryption, privacy-focused services, and informed choices about what you share.

+ [Learn about specific topics]
    -> Explore_Topics

+ [Start over]
    -> Start

+ [Exit]
    ~ exit()
    -> DONE

=== Explore_Topics ===

# diagram: ../childrens-wellbeing-bill/bill_overview.jpg

Each topic has more depth to explore:

+ [Digital ID and surveillance]
    ~ navigateTo("digital-id")
    -> DONE

+ [Facial recognition]
    ~ navigateTo("facial-recognition")
    -> DONE

+ [Age verification]
    ~ navigateTo("age-verification")
    -> DONE

+ [On-device scanning]
    ~ navigateTo("on-device-scanning")
    -> DONE

+ [Children's Wellbeing Bill]
    ~ navigateTo("childrens-wellbeing-bill")
    -> DONE

+ [Back to start]
    -> Start
